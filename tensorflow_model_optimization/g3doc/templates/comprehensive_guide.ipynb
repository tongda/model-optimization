{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_comprehensive_guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbORZA_bQx1G",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the comprehensive guide for Keras weight pruning. \n",
        "\n",
        "Use this page to quickly find the APIs you need for your use case via the navigation sidebar. Once you know which APIs you need, find the parameters and the low-level details in the\n",
        "[API docs](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity). \n",
        "\n",
        "*  If you want to see the benefits of pruning and what's supported, see the [pruning overview](https://www.tensorflow.org/model_optimization/guide/pruning). \n",
        "*  For a single end-to-end example, see the [pruning example](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras).\n",
        "\n",
        "The following corresponds to the navigation sidebar:\n",
        "* Without a pruned model, you must **define** and **train** the model.\n",
        "* For Keras HDF5 models only, you need special **checkpointing and deserialization**. Checkpointing cannot be done with Keras HDF5 weights.\n",
        "* For **deployment** only, you must take steps to see compression benefits.\n",
        "\n",
        "For configuration of the pruning algorithm, refer to the [prune_low_magnitude\n",
        "API docs](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude).\n",
        "\n",
        "Run the boilerplate code below once to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuABqZnXVDvO",
        "colab_type": "text"
      },
      "source": [
        "# Boilerplate: run once per Colab session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvpH1Hg7ULFz",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "4cbed68b-4bfc-4cda-d0db-34b50d432ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Run this section once per Colab session.\n",
        "\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -q tensorflow==2.1.0\n",
        "! pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "\n",
        "import tempfile\n",
        "\n",
        "input_shape = [20]\n",
        "x_train = np.random.randn(1, 20).astype(np.float32)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n",
        "\n",
        "def setup_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(20, input_shape=input_shape),\n",
        "      tf.keras.layers.Flatten()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def setup_pretrained_weights():\n",
        "  model = setup_model()\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  _, pretrained_weights = tempfile.mkstemp('.h5')\n",
        "\n",
        "  model.save_weights(pretrained_weights)\n",
        "\n",
        "  return pretrained_weights\n",
        "  \n",
        "def get_gzipped_model_size(model):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, keras_file = tempfile.mkstemp('.h5') \n",
        "  model.save(keras_file, include_optimizer=False)\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip') \n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(keras_file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)\n",
        "\n",
        "setup_model()\n",
        "pretrained_weights = setup_pretrained_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hTrain on 1 samples\n",
            "1/1 [==============================] - 0s 479ms/sample - loss: 1.5926 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZyLYFTER4aP"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybigft1fTn4T",
        "colab_type": "text"
      },
      "source": [
        "### Prune all layers in Functional and Sequential models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZvqnp1xsn-",
        "colab_type": "text"
      },
      "source": [
        "**Tips** for better model accuracy:\n",
        "\n",
        "* Try \"Prune some layers\" on the navigation sidebar to skip pruning the layers that affect accuracy the most.\n",
        "* Generally better to start from pre-trained weights.\n",
        "\n",
        "**More**: the [`prune_low_magnitude`](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) API docs provide details on configuring\n",
        "the pruning algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIn-hFO_T_PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = setup_model()\n",
        "model.load_weights(pretrained_weights) # optional but recommended.\n",
        "\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTbTLn3dZM7h",
        "colab_type": "text"
      },
      "source": [
        "### Prune some layers in Functional and Sequential models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbM8o832xTxV",
        "colab_type": "text"
      },
      "source": [
        "**Tips** for better model accuracy:\n",
        "\n",
        "* Generally better to start from pre-trained weights.\n",
        "* Try pruning the later layers instead of the first layers.\n",
        "* Avoid pruning critical layers (e.g. attention mechanism). \n",
        "\n",
        "**More**: the [`prune_low_magnitude`](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) API docs provide details on how to vary the pruning configuration per layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN0B_QB-ZhE2",
        "colab_type": "code",
        "outputId": "1bd54254-e145-4b04-d3de-f09d2c282b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "model = setup_model()\n",
        "model.load_weights(pretrained_weights) # optional but recommended\n",
        "\n",
        "## Version 1: Prune all dense layers.\n",
        "def apply_pruning_to_dense(layer):\n",
        "  if isinstance(layer, tf.keras.layers.Dense):\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
        "  return layer\n",
        "    \n",
        "pruned_dense_layers_model = tf.keras.models.clone_model(\n",
        "    model, \n",
        "    clone_function=apply_pruning_to_dense,\n",
        ")\n",
        "\n",
        "print(\"pruned_dense_layers\")\n",
        "pruned_dense_layers_model.summary()\n",
        "\n",
        "## Version 2: Prune the first layer, achieving the same result.\n",
        "def layers_to_prune():\n",
        "  # Knowing that the first layer is the Dense layer.\n",
        "  return {model.layers[0]: 'default'}\n",
        "\n",
        "def apply_pruning_to_first(layer):\n",
        "  if layer in layers_to_prune():\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
        "  return layer\n",
        "\n",
        "pruned_first_layer_model = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function = apply_pruning_to_first\n",
        ")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"pruned_first_layer\")\n",
        "pruned_first_layer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pruned_dense_layers\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_dense_6  (None, 20)                822       \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 822\n",
            "Trainable params: 420\n",
            "Non-trainable params: 402\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "pruned_first_layer\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_dense_6  (None, 20)                822       \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 822\n",
            "Trainable params: 420\n",
            "Non-trainable params: 402\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb_BydRaSoF",
        "colab_type": "text"
      },
      "source": [
        "#### More readable but potentially less accurate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vqXeYffzSHp",
        "colab_type": "text"
      },
      "source": [
        "This is not compatible with using pre-trained weights, which is why it may be less accurate.\n",
        "\n",
        "**More**: the [`prune_low_magnitude`](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) API docs provide details on how to vary the pruning configuration per layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQoMH3g3fWwb",
        "colab_type": "text"
      },
      "source": [
        "Functional example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wow55hg5oiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = tf.keras.Input(shape=(20,))\n",
        "x = tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(10))(i)\n",
        "o = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "pruned_model = tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIGj-r2of2ls",
        "colab_type": "text"
      },
      "source": [
        "Sequential example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQOiDUGgfi4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pruned_model = tf.keras.Sequential([\n",
        "  tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(20, input_shape=input_shape)),\n",
        "  tf.keras.layers.Flatten()\n",
        "])\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eLdbDOT5q3F",
        "colab_type": "text"
      },
      "source": [
        "## Prune layers in Subclassed model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apwQ2zzrzTkp",
        "colab_type": "text"
      },
      "source": [
        "**Note**: using pre-trained weights is not supported yet.\n",
        "\n",
        "**Tips** for better model accuracy:\n",
        "* Trying pruning the later layers instead of the first layers\n",
        "* Avoid pruning critical layers (e.g. attention mechanism). \n",
        "\n",
        "**More**: the [`prune_low_magnitude`](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) API docs provide details on how to vary the pruning configuration per layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABVVVCmJ5ucw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPrunedModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyPrunedModel, self).__init__()\n",
        "    self.dense = tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(10))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense2 = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        tf.keras.Sequential([tf.keras.layers.Dense(10)])\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense(inputs)\n",
        "    x = self.flatten(x)\n",
        "    return self.dense2(x)\n",
        "\n",
        "pruned_model = MyPrunedModel()\n",
        "\n",
        "input_shape = (None, 20)\n",
        "pruned_model.build(input_shape)\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnMguvVSnUqD",
        "colab_type": "text"
      },
      "source": [
        "## Prune custom Keras layer or prune different weights from API default "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgH1aFMjTK4",
        "colab_type": "text"
      },
      "source": [
        "**Common mistake:** pruning the bias usually harms model accuracy too much.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77jgBjccnTh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDenseLayer(tf.keras.layers.Dense, tfmot.sparsity.keras.PrunableLayer):\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    # Prune bias also, though that usually harms model accuracy too much.\n",
        "    return [self.kernel, self.bias]\n",
        "\n",
        "class MyDenseLayer2(tf.keras.layers.Dense, tfmot.sparsity.keras.PrunableLayer):\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    # Prune nothing.\n",
        "    return []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itAyTyzvRroH",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4hnWH2NY5MO",
        "colab_type": "text"
      },
      "source": [
        "## Model.fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKZ2PxcpY_WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See \"Define model\" on navigation sidebar for how to prune this model\n",
        "# in more other ways.\n",
        "model = setup_model()\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "\n",
        "log_dir = tempfile.mkdtemp()\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    # Log sparsity and other metrics in Tensorboard.\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)\n",
        "]\n",
        "\n",
        "pruned_model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        ")\n",
        "\n",
        "pruned_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "%tensorboard --logdir={log_dir}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDcSvbNdZA-1",
        "colab_type": "text"
      },
      "source": [
        "## Custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPQUrkodbIF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See \"Define model\" on navigation sidebar for how to prune this model\n",
        "# in more other ways.\n",
        "model = setup_model()\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "\n",
        "# Boilerplate\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "log_dir = tempfile.mkdtemp()\n",
        "unused_arg = -1\n",
        "epochs = 1\n",
        "batches = 1 # example is hardcoded so that the number of batches cannot change.\n",
        "\n",
        "# Non-boilerplate.\n",
        "pruned_model.optimizer = optimizer\n",
        "step_callback = tfmot.sparsity.keras.UpdatePruningStep()\n",
        "step_callback.set_model(pruned_model)\n",
        "log_callback = tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir) # Log sparsity and other metrics in Tensorboard.\n",
        "log_callback.set_model(pruned_model)\n",
        "\n",
        "step_callback.on_train_begin() # run pruning callback\n",
        "for _ in range(epochs):\n",
        "  for _ in range(batches):\n",
        "    step_callback.on_train_batch_begin(batch=unused_arg) # run pruning callback\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = pruned_model(x_train, training=True)\n",
        "      loss_value = loss(y_train, logits)\n",
        "      grads = tape.gradient(loss_value, pruned_model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, pruned_model.trainable_variables))\n",
        "\n",
        "  step_callback.on_epoch_end(batch=unused_arg) # run pruning callback\n",
        "  log_callback.on_epoch_end(batch=unused_arg) # run pruning callback\n",
        " \n",
        "\n",
        "%tensorboard --logdir={log_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8H-8lQ-cPa-",
        "colab_type": "text"
      },
      "source": [
        "## Improve pruned model accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2t4fYXvAV1V",
        "colab_type": "text"
      },
      "source": [
        "First, look at the [prune_low_magnitude\n",
        "API docs](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude)\n",
        "to understand what a pruning schedule is and the math of\n",
        "each type of pruning schedule.\n",
        "\n",
        "**Tips**: \n",
        "\n",
        "* Have a learning rate that's not too high or too low when the model is pruning. Consider the [pruning schedule](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruningSchedule) to be a hyperparameter.\n",
        "\n",
        "* As a quick test, try running an experiment where you prune a model to the final sparsity with begin step 0 with a\n",
        "[ConstantSparsity](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/ConstantSparsity) schedule. You might get lucky with good results.\n",
        "\n",
        "* Do not prune very frequently to give the model time to recover. The [pruning schedule](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruningSchedule) provides a decent default frequency.\n",
        "\n",
        "* For general ideas to improve model accuracy,\n",
        "find your use case(s) under \"Define model\" on the navigation sidebar and see if there are tips."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpvX5IqahV1r",
        "colab_type": "text"
      },
      "source": [
        "# Checkpointing and Deserialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZ5wlij1dcJ",
        "colab_type": "text"
      },
      "source": [
        "**Your Use Case:**\n",
        "\n",
        "* You cannot do checkpointing with Keras HDF5 weights since we need to preserve the step.\n",
        "\n",
        "* This code is only needed for the HDF5 model format (not HDF5 weights or other formats)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khQg-q7imfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See \"Define model\" on navigation sidebar for how to prune this model\n",
        "# in more other ways.\n",
        "model = setup_model()\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "\n",
        "_, keras_model_file = tempfile.mkstemp('.h5')\n",
        "# Saving the optimizer is necessary for checkpointing (True is the default).\n",
        "pruned_model.save(keras_model_file, include_optimizer=True)\n",
        "\n",
        "with tfmot.sparsity.keras.prune_scope():\n",
        "  loaded_model = tf.keras.models.load_model(keras_model_file)\n",
        "\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jew8M217SgQw",
        "colab_type": "text"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uj4SfF1cnTR",
        "colab_type": "text"
      },
      "source": [
        "## Export model with size compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57uNm47L4Yro",
        "colab_type": "text"
      },
      "source": [
        "**Common mistake**: both `strip_pruning` and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression\n",
        "benefits of pruning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ3TD8cYkxZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See \"Define model\" and \"Train model\" on navigation sidebar for how to define\n",
        "# and train this model in other ways.\n",
        "model = setup_model()\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "\n",
        "pruned_model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        ")\n",
        "\n",
        "pruned_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()]\n",
        ")\n",
        "\n",
        "final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "print(\"final model\")\n",
        "final_model.summary()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Size of gzipped pruned model without stripping: %.2f bytes\" % (get_gzipped_model_size(pruned_model)))\n",
        "print(\"Size of gzipped pruned model with stripping: %.2f bytes\" % (get_gzipped_model_size(final_model)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPXvYIHOctem",
        "colab_type": "text"
      },
      "source": [
        "## Hardware-specific optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqk0jI49c1mw",
        "colab_type": "text"
      },
      "source": [
        "Once the framework [enables pruning to improve latency]((https://github.com/tensorflow/model-optimization/issues/173)), using block sparsity can improve latency for certain hardware. For a target model accuracy, latency can still improve despite the fact that increasing the block size will\n",
        "decrease the peak sparsity %."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xedaVDeFc0bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = setup_model()\n",
        "\n",
        "# For using intrinsics on a CPU with 128-bit registers, together with 8-bit\n",
        "# quantized weights, a 1x16 block size is nice because the block perfectly\n",
        "# fits into the register.\n",
        "pruning_params = {'block_size': [1, 16]}\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}